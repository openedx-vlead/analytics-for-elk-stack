#+TITLE: Configuring Elasticsearch Service
#+AUTHOR: VLEAD
#+DATE: [2016-06-07 Tue]
#+SETUPFILE: ../org-templates/level-1.org
#+TAGS: boilerplate(b)
#+EXCLUDE_TAGS: boilerplate
#+OPTIONS: ^:nil

* Introduction
  This document will illustrate the configuration required for Nginx to run
  elasticsearch service on port =80= on development environment using =vagrant=
  

* Steps to run =elasticsearch= service behind =nginx= on port =80=
- [[https://github.com/vlead/vlead-templates/blob/develop/vagrant-boxes/export-vagrant-box.org#steps-to-create-vm-using-vagrant][Create VM]] using vagrant 
- Update VM
#+BEGIN_EXAMPLE
sudo apt-get update -y
#+END_EXAMPLE
- Install the software-properties-common Package
#+BEGIN_EXAMPLE
sudo apt-get install software-properties-common python-software-properties -y
#+END_EXAMPLE
- To install =java8=, run
#+BEGIN_EXAMPLE
sudo apt-add-repository ppa:webupd8team/java -y
sudo apt-get update -y
echo 'oracle-java8-installer shared/accepted-oracle-license-v1-1 select true' | sudo debconf-set-selections
sudo apt-get install oracle-java8-installer -y
#+END_EXAMPLE
- Install elasticsearch =v1.7.2=
#+BEGIN_EXAMPLE
wget https://download.elastic.co/elasticsearch/elasticsearch/elasticsearch-1.7.2.deb && dpkg -i elasticsearch-1.7.2.deb
#+END_EXAMPLE

- Start Elasticsearch
#+BEGIN_EXAMPLE
sudo service elasticsearch start
#+END_EXAMPLE
- Install nginx web server
#+BEGIN_EXAMPLE
sudo apt-get install nginx -y
#+END_EXAMPLE
- Start nginx
#+BEGIN_EXAMPLE
sudo service nginx start
#+END_EXAMPLE
- Routing the requests through nginx
  + First lets disable Elasticsearch from receiving external requests. In the
    elasticsearch server, open the file /etc/elasticsearch/elasticsearch.yml
    for editting. Comment out the config network.bind_host and
    network.publish_host.
#+BEGIN_EXAMPLE
#network.bind_host: #some_value
#network.publish_host: #some_other_value 
network.host: localhost
#+END_EXAMPLE
- Check by restarting service
#+BEGIN_EXAMPLE
sudo service elasticsearch restart
#+END_EXAMPLE
- Run elasticsearch behind nginx
  + To accomplish that, we need to create a file
    /etc/nginx/sites-available/elasticsearch with the following content.
#+BEGIN_EXAMPLE
server {
    listen 80;
    server_name localhost;
    location / {
    limit_except GET {
      auth_basic           "Login required";
      auth_basic_user_file /etc/nginx/.htpasswd;
    } 
        rewrite ^/(.*) /$1 break;
        proxy_ignore_client_abort on;
        proxy_pass http://localhost:9200;
        proxy_redirect http://localhost:9200 http://localhost/;
        proxy_set_header  X-Real-IP  $remote_addr;
        proxy_set_header  X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header  Host $http_host;
    }
}

#+END_EXAMPLE
- Setup credentials
#+BEGIN_EXAMPLE
htpasswd -c /etc/nginx/.htpasswd username
#+END_EXAMPLE
- Create symbolic link to nginx virtual conf file =elasticsearch=
#+BEGIN_EXAMPLE
sudo ln /etc/nginx/sites-available/elasticsearch /etc/nginx/sites-enabled/
#+END_EXAMPLE
- Remove =default= file from =nginx=
#+BEGIN_EXAMPLE
rm /etc/nginx/sites-enabled/default
#+END_EXAMPLE
- Reload & restart nginx
#+BEGIN_EXAMPLE
sudo service nginx reload
sudo service nginx restart
#+END_EXAMPLE
- Run =nginx= and =elasticsearch= on boot time
#+BEGIN_EXAMPLE
sudo update-rc.d nginx defaults
sudo update-rc.d elasticsearch defaults
#+END_EXAMPLE
- Access elasticsearch on browser
#+BEGIN_EXAMPLE
firefox http://localhost
#+END_EXAMPLE


* Script to migrate the elasticsearch database

  1. Script takes the backup of all existing =indexes= and store them into the
     =.json= file with naming convention as =index_type.json=
  2. Also it migrates the dump of above =step1= data into new elasticsearch
     database as per new [[https://github.com/vlead/analytics-db/blob/refactor/src/design/index.org#design-of-elasticsearch-database][design]]
  3. This script uses [[http://lds.vlabs.ac.in][LDS]] service to make the records consistent before storing
     into database.
** Function to backup the elasticsearch database
   - Function takes the backup of all existing =indexes= and store them into
     the =.json= file with naming convention as =index_type.json=
   - This script will backup the openedx usage analytics from =2016-10-07= to
     till date
#+NAME: backup
#+BEGIN_SRC python

ELASTIC_DB_URL = "http://192.168.33.10"
LDS_URL = "http://lds.vlabs.ac.in"
file_names = []


def backup():
    headers = {'Content-Type': 'application/json'}
    GET_INDEXS_API = "%s/_cat/indices" % (ELASTIC_DB_URL)

    try:
        r = requests.get(GET_INDEXS_API, headers=headers)
        if r.status_code == 200:
            indexes = r.json()
        else:
            print "Failed to get index lists"
            
    except Exception as e:
        print str(e)

    for index in indexes:
        index_name = index['index']
        GET_TYPES_API = "%s/%s" % (ELASTIC_DB_URL, index_name)

        try:
            r = requests.get(GET_TYPES_API)
            if r.status_code == 200:
                types = r.json()[index_name]['mappings'].keys()
                for type in types:
                    temp = []

                    if index_name == "vlabs" and type == "usage":
                        
                        start_date = date(2016, 10, 07)
                        end_date = date(2017, 8, 31)
                        for dt in rrule(DAILY, dtstart=start_date, until=end_date):
                            t_date = dt.strftime("%Y-%m-%d")
                            GET_DOCS_API = "%s/%s/%s/_search?q=DATE_OF_EXPERIMENT:%s&size=10000" %(ELASTIC_DB_URL, index_name, str(type), t_date)
                            r = requests.get(GET_DOCS_API)
                            if r.status_code == 200:
                                docs = r.json()['hits']['hits']
                                for doc in docs:
                                    temp.append(doc['_source'])
                    else:
                        GET_DOCS_API = "%s/%s/%s/_search?size=10000" %(ELASTIC_DB_URL, index_name, str(type))
                        r = requests.get(GET_DOCS_API)
                        if r.status_code == 200:
                            docs = r.json()['hits']['hits']
                            for doc in docs:
                                temp.append(doc['_source'])
                    file_name = "%s_%s.json" % (index_name, str(type))
                    file_names.append(file_name)
                    try:
                        f = open(file_name, 'w')
                        json.dump(temp, f)
                        f.close()
                    except Exception as e:
                        print str(e)

            else:
                print "Failed to get types list"

        except Exception as e:
            print str(e)
#+END_SRC
** Configure string analyzer
#+NAME: config_string_analyzer
#+BEGIN_SRC python
            
def config_string_analyzer(index):
    ANALYTICS_API = "%s/%s" % (ANALYTICS_URL, index)
    data = {
        "index" : {
            "analysis" : {
                "analyzer" : {
                    "default" : {
                        "type" : "keyword"
                        }
                    }
                }
            }
        }
    r = requests.put(ANALYTICS_API, data=json.dumps(data), headers=headers)
    if r.status_code == 200:
        return True
    else:
        return False
   

#+END_SRC
** Migrate json records
   This functions restores the =json= records into specified =index= and =type=
#+NAME: migrate_json_records
#+BEGIN_SRC python
            
def migrate_json_records(records, index, type):
    config_string_analyzer(index)
    count = 0
    ELASTIC_DB_API = "%s/%s/%s" % (ELASTIC_DB_URL, index, type)
    headers = {'Content-Type': 'application/json'}
    for record in records:
        if 'mac_addr' in record:
            ELASTIC_DB_API = "%s/%s/%s/%s" % (ELASTIC_DB_URL, index, type, record['mac_addr'])

        try:
            r = requests.post(ELASTIC_DB_API, data=json.dumps(record), headers=headers)
            if r.status_code == 201:
                count = count + 1
                print "************************************"
                print ELASTIC_DB_API
                print "Added record %s/%s" % (count, len(records))
                print "************************************"
            else:
                print "failed to add record %s" % (record)

        except Exception as e:
            print str(e)
#+END_SRC

** Migrate usage
   - This function migrates the usage records into specified =index= and =type=
     also it contacts =LDS= to make the usage records consistent such as
     =lab-name= and =exp-name=.
   - This script will migrate the openedx usage analytics from =2016-10-07= to till date
#+NAME: migrate_usage
#+BEGIN_SRC python

def migrate_usage(usages, index, type):
    config_string_analyzer(index)
    count = 0
    ELASTIC_DB_API = "%s/%s/%s" % (ELASTIC_DB_URL, index, type)
    usage_dict = {}
    headers = {'Content-Type': 'application/json'}
    for usage in usages:
        usage_dict['lab_id'] = usage['LAB_ID'].lower()
        usage_dict['date_of_experiment'] = usage['DATE_OF_EXPERIMENT']
        usage_dict['student_id'] = usage['STUDENT_ID']
        usage_dict['region'] = usage['REGION']
        LDS_API = "%s/labs?lab_id=%s" % (LDS_URL, usage_dict['lab_id'])

        try:
            r = requests.get(LDS_API, headers=headers)
            if r.status_code == 200:
                usage_dict['lab_name'] = r.json()['lab_name']
                usage_dict['discipline_name'] = r.json()['discipline']['discipline_name']
                usage_dict['institute_name'] = r.json()['institute']['institute_name']
            else:
                usage_dict['lab_name'] = usage['LAB_NAME']

        except Exception as e:
            print str(e)
            
        usage_dict['experiment_id'] = usage['EXPERIMENT_ID'].lower()
        LDS_API = "%s/experiments?exp_id=%s" % (LDS_URL, usage_dict['experiment_id'])

        try:
            r = requests.get(LDS_API, headers=headers)
            if r.status_code == 200:
                usage_dict['experiment_name'] = r.json()['exp_name']
            else:
                usage_dict['experiment_name'] = usage['EXPERIMENT_NAME']
        except Exception as e:
            print str(e)

        usage_dict['time_of_experiment'] = usage['TIME_OF_EXPERIMENT']
        usage_dict['ip_address'] = usage['IP_ADDRESS']

        try:
            r = requests.post(ELASTIC_DB_API, data=json.dumps(usage_dict), headers=headers)
            if r.status_code == 201:
                count = count + 1
                print "************************************"
                print ELASTIC_DB_API
                print "Added record %s/%s" % (count, len(usages))
                print "************************************"            

            else:
                print "failed to add usage record %s" % (usage_dict)

        except Exception as e:
            print str(e)
#+END_SRC

** Migrate db
   This function reads all =.json= files which are obtained in backup step and
   invokes necessary functions to restore them into elasticsearch database.
#+NAME: migrate_db
#+BEGIN_SRC python

def migrate():

    for file_name in file_names:
        if file_name == "vlabs_usage.json":
            index = "vlabs"
            type = "openedx_usage"

            try:
                with open(file_name) as data_file:
                    usages = json.load(data_file)
                migrate_usage(usages, index, type)

            except Exception as e:
                print str(e)

        if file_name == "college_cloud_details.json":
            index = "college_cloud"
            type = "details"

            try:
                with open(file_name) as data_file:
                    cc_details = json.load(data_file)
                migrate_json_records(cc_details, index, type)

            except Exception as e:
                print str(e)
                
        if file_name == "matrusri-college_70:54:d2:7b:3d:70_usages.json":
            index = "college_cloud"
            type = "matrusri-college_70:54:d2:7b:3d:70_usage"

            try:
                with open(file_name) as data_file:
                    cc_usages = json.load(data_file)
                migrate_usage(cc_usages, index, type)

            except Exception as e:
                print str(e)
                
        if file_name == "matrusri-college_70:54:d2:7b:3d:70_feedback.json":
            index = "college_cloud"
            type = "matrusri-college_70:54:d2:7b:3d:70_feedback"

            try:
                with open(file_name) as data_file:
                    feedbacks = json.load(data_file)
                migrate_json_records(feedbacks, index, type)

            except Exception as e:
                print str(e)
                
        if file_name == "juit_70:54:d2:7b:3d:36_feedback.json":
            index = "college_cloud"
            type = "juit_70:54:d2:7b:3d:36_feedback"
            ELASTIC_DB_API = "%s/%s/%s" % (ELASTIC_DB_URL, index, type)

            try:
                with open(file_name) as data_file:
                    feedbacks = json.load(data_file)
                migrate_json_records(feedbacks, index, type)

            except Exception as e:
                print str(e)
        if file_name == "juit_70:54:d2:7b:3d:36_usages.json":
            index = "college_cloud"
            type = "juit_70:54:d2:7b:3d:36_usage"

            try:
                with open(file_name) as data_file:
                    cc_usages = json.load(data_file)
                migrate_usage(cc_usages, index, type)

            except Exception as e:
                print str(e)

#+END_SRC
** main function
   This function invokes =backup()= or =migrate()= functions.
#+NAME: main
#+BEGIN_SRC python
if __name__== "__main__":
    backup()
    #migrate()

#+END_SRC
  

* Important commands for dealing with Elasticsearch database.
  
** Start/Stop/Check status of elasticsearch service
   - Generic command : service elasticsearch <start/stop/restart/status>
   #+BEGIN_EXAMPLE
   service elasticsearch start
   #+END_EXAMPLE


** Create an index in elasticsearch 
   - An index can be created using Elasticsearch API
   - Invoke API using curl command or by using python-elasticsearch client.
   - Generic command  : curl -XPOST "http://<host-name>:<port-number>/<index-name>"
    #+BEGIN_EXAMPLE
    root@vlabs-analytics:~# curl -XPOST "http://localhost:9200/test-index"
    #+END_EXAMPLE
   - If index is successfully created you get a True ACK from elasticsearch
     server.
    #+BEGIN_EXAMPLE
    {"acknowledged":true}
    #+END_EXAMPLE


** Insert a record in elasticsearch database 
   - Generic command : curl -XPOST "http://http://<hostname>:<port-number>/<index-name>/<doc-type>/[id-of-record]" -d 
                        {
                         "key1" : "value1",
                         "KeyN" : "valueN"
                        }'
     #+BEGIN_EXAMPLE
     root@vlabs-analytics:~# curl -XPOST "http://localhost:9200/test-index/document-test/" -d '{"name":"Sripathi", "age":"23"}'
     #+END_EXAMPLE
   - If record is sucessfull inserted you get a ACK as shown An ID for each
     record is dynamically generated, if it is not mentioned while inserting
     data.
    #+BEGIN_SRC command
    {
    "_index":"test-index",
    "_type":"document-test",
    "_id":"AViVv6dqvXkOVLf8Bz4E",
    "_version":1,
    "_shards":{"total":2,"successful":1,"failed":0},
    "created":true
    }
    #+END_SRC


** Check the status of elasticsearch using curl command
   - Generic command : curl http://<hostname>:<port-number> 

    #+BEGIN_EXAMPLE
    root@vlabs-analytics:~# curl http://localhost:9200/
    #+END_EXAMPLE
   - Above comand gives version information, cluster build time, cluster name
     of elastisearch. See given output is given below -

    #+BEGIN_EXAMPLE
    {
      "name" : "Janus",
      "cluster_name" : "elasticsearch",
      "cluster_uuid" : "aUdpyb8BR3ieeLEj3q5Z7Q",
      "version" : {
        "number" : "2.4.1",
        "build_hash" : "c67dc32e24162035d18d6fe1e952c4cbcbe79d16",
        "build_timestamp" : "2016-09-27T18:57:55Z",
        "build_snapshot" : false,
        "lucene_version" : "5.5.2"
      },
      "tagline" : "You Know, for Search"
    }

    #+END_EXAMPLE


** List the existing indexes in elasticsearch database 
   - Generic command : curl http://<hostname>:<port-number>/_cat/indices?v
    #+BEGIN_EXAMPLE command
    root@vlabs-analytics:~# curl http://localhost:9200/_cat/indices?v
    #+END_EXAMPLE
   - Output for above command is

    #+BEGIN_EXAMPLE
    health status index   pri rep docs.count docs.deleted store.size pri.store.size 
    yellow open   .kibana   1   1          1            0      3.1kb          3.1kb 
    yellow open   vlabs     5   1          0            0       795b           795b
    #+END_EXAMPLE


** Fetch records from index
   - To obtain data from elasticsearch use the command as shown
    #+BEGIN_EXAMPLE
    root@vlabs-analytics:~# curl -XGET "http://<hostname>:<port-number>/<index-name>/_search?pretty&size=<No-of-records>"
    root@vlabs-analytics:~# curl -XGET "http://localhost:9200/vlabs/_search?pretty&size=2"
    #+END_EXAMPLE
    Output obtained as given below -
    #+BEGIN_EXAMPLE
    {
      "took" : 1,
      "timed_out" : false,
      "_shards" : {
        "total" : 5,
        "successful" : 5,
        "failed" : 0
      },
      "hits" : {
        "total" : 2671,
        "max_score" : 1.0,
        "hits" : [ {
          "_index" : "vlabs",
          "_type" : "usage",
          "_id" : "AVee00ocYbJBYuvGUfjp",
          "_score" : 1.0,
          "_source" : {
            "LAB_ID" : "EEE06",
            "DATE_OF_EXPERIMENT" : "2016-10-07",
            "STUDENT_ID" : "student",
            "REGION" : "Telangana",
            "LAB_NAME" : "Virtual Power Lab",
            "EXPERIMENT_NAME" : "To Study the over-current relay and the effect of PSM and TSM",
            "EXPERIMENT_ID" : "E99850",
            "TIME_OF_EXPERIMENT" : "11:07",
            "IP_ADDRESS" : "14.139.82.6"
          }
        }, {
          "_index" : "vlabs",
          "_type" : "usage",
          "_id" : "AVeew8TSYbJBYuvGUfjP",
          "_score" : 1.0,
          "_source" : {
            "LAB_ID" : "CHEM01",
            "DATE_OF_EXPERIMENT" : "2016-10-07",
            "STUDENT_ID" : "student",
            "REGION" : "Telangana",
            "LAB_NAME" : "Chemical Engineering Lab",
            "EXPERIMENT_NAME" : "Flow measurement by orificemeter and venturimeter",
            "EXPERIMENT_ID" : "E99656",
            "TIME_OF_EXPERIMENT" : "10:50",
            "IP_ADDRESS" : "14.139.82.6"
          }
        } ]
      }
    }

    #+END_EXAMPLE


** Delete an index in elasticsearch 
   - Deleting an index will delete all documents under that index.  To DELETE
     an index, use the following command -

    #+BEGIN_EXAMPLE
    General Syntax # curl -XDELETE "http://<hostname>:<port-number>/<index-name>"
    root@vlabs-analytics:~# curl -XDELETE "http://localhost:9200/test-index"
    #+END_EXAMPLE
   - Once deleted, an ACK is given as shown below -
     #+BEGIN_EXAMPLE command
     {"acknowledged":true}
     #+END_EXAMPLE

     
* Configuring string analyzer for Elasticsearch 
  - Elasticsearch has default settings to analyze a string. For example
    see the example document given below -
    #+BEGIN_EXAMPLE
    {
    "name" : "Sripathi Kammari",
    "age" : "23" 
    }
    #+END_EXAMPLE

  - When visualizations are generated in kibana, you see different graphs for
    "Sripathi" and "Kammari" even though it is a single value assigned to
    "name" key.

  - To Override this default behavior, custom settings are done while creating
    an index.  To disable default string analyzer use the general syntax and
    example as shown -

    #+BEGIN_EXAMPLE

    curl -XPUT http://<hostname>:<port-number>/<index-name> -d '{
        "index" : {
            "analysis" : {
                "analyzer" : {
                    "default" : {
                        "type" : "keyword"
                    }
                }
            }
        }
    }'
    #+END_EXAMPLE
  - The above command will create an index which does not exist and overrite
    the settings to disable analyzer.

  - A example using the above command is -
    #+BEGIN_SRC command
    curl -XPUT localhost:9200/test-index -d '{
        "index" : {
            "analysis" : {
                "analyzer" : {
                    "default" : {
                        "type" : "keyword"
                    }
                }
            }
        }
    }'
    #+END_SRC


* Query DSL for elasticsearch 
** Fetch records matching the specific string from a field
   - Given below is the syntax to serach for a specific string in a specific
     field.


   #+BEGIN_EXAMPLE
   GET /<index-name>/<doc-type>/_search?pretty&size=<no-of-records>
   {
   "query" : {
                        "term" : { 
                    "<Field-name>" : "<field value to search for>"
                }
            }
        }
   #+END_EXAMPLE
   Example run is given below -  
   #+BEGIN_EXAMPLE
  GET /vlabs/usage/_search?pretty&size=50
   {
    "query" : {
       "term" : { 
                    "REGION" : "Telangana"
                }
            }
        }
    
   #+END_EXAMPLE
   Hence a search based on the string value of a filed in record is done 
   using =term= keyword of Elasticsearch Query DSL

** To fetch analytics for a specific date 
   Use the below query-
   
   #+BEGIN_EXAMPLE
   GET /vlabs/usage/_search?pretty&size=50
   {   
       "query" : {
       "term" : { 
              "DATE_OF_EXPERIMENT" : "2016-10-07"
               }
             }
   }
   #+END_EXAMPLE
   Returns all the records added having date mentioned.
   Sample output is given below for 3 records 

    #+BEGIN_EXAMPLE
    {
    "took": 7,
    "timed_out": false,
    "_shards": {
      "total": 5,
      "successful": 5,
      "failed": 0
    },
    "hits": {
      "total": 87,
      "max_score": 4.942464,
      "hits": [
        {
          "_index": "vlabs",
          "_type": "usage",
          "_id": "AVefLTSmYbJBYuvGUfkx",
          "_score": 4.942464,
          "_source": {
            "LAB_ID": "MECH48",
            "DATE_OF_EXPERIMENT": "2016-10-07",
            "STUDENT_ID": "student",
            "REGION": "Telangana",
            "LAB_NAME": "Virtual Combustion and Atomization Laboratory",
            "EXPERIMENT_NAME": "Numerical characterization of laminar premixed methane-air flames",
            "EXPERIMENT_ID": "E99433",
            "TIME_OF_EXPERIMENT": "12:46",
            "IP_ADDRESS": "14.139.82.6"
          }
        },
        {
          "_index": "vlabs",
          "_type": "usage",
          "_id": "AVee7i6hYbJBYuvGUfkA",
          "_score": 4.942464,
          "_source": {
            "LAB_ID": "ECE40",
            "DATE_OF_EXPERIMENT": "2016-10-07",
            "STUDENT_ID": "student",
            "REGION": "Telangana",
            "LAB_NAME": "Electronic Devices and Circuits",
            "EXPERIMENT_NAME": "I-V Characteristics and Fabrication of p-n junction Diode",
            "EXPERIMENT_ID": "E99646",
            "TIME_OF_EXPERIMENT": "11:37",
            "IP_ADDRESS": "14.139.82.6"
          }
        },
        {
          "_index": "vlabs",
          "_type": "usage",
          "_id": "AVefEWpPYbJBYuvGUfkh",
          "_score": 4.942464,
          "_source": {
            "LAB_ID": "BIOTECH25",
            "DATE_OF_EXPERIMENT": "2016-10-07",
            "STUDENT_ID": "student",
            "REGION": "Telangana",
            "LAB_NAME": "Virtual Proteomics Laboratory",
            "EXPERIMENT_NAME": "In-gel digestion of proteins for MS analysis",
            "EXPERIMENT_ID": "E99613",
            "TIME_OF_EXPERIMENT": "12:15",
            "IP_ADDRESS": "14.139.82.6"
          }
        }
      ]
     }
    }
    #+END_EXAMPLE


* References
  + https://www.elastic.co/products/elasticsearch
  + [[http://www.elasticsearchtutorial.com/elasticsearch-in-5-minutes.html][Elasticsearch in 5 mins]]
  + [[http://1.droppdf.com/files/FOeNs/elasticsearch-the-definitive-guide-clinton-gormley-zachary-tong.pdf][Elasticsearch Definitive Guide]]
  + [[https://www.elastic.co/guide/en/elasticsearch/guide/current/analysis-intro.html][Elasticsearch String analyzer]]


* Tangle                                                        :boilerplate:
*** Imports 

#+NAME: imports
#+BEGIN_SRC python
import requests
import json
from datetime import datetime, timedelta, date
from dateutil.rrule import rrule, DAILY

#+END_SRC

*** Sources
#+BEGIN_SRC python :tangle migrate_elastic_db.py :eval no :noweb yes

<<imports>>
<<backup>>
<<config_string_analyzer>>
<<migrate_usage>>
<<migrate_json_records>>
<<migrate_db>>
<<main>>
#+END_SRC
